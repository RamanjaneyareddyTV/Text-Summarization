{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Abstract\n",
    "2. Dataset\n",
    "3. Goal\n",
    "4. Importing the required libraries and dataset\n",
    "5. Data Pre-processing\n",
    "6. Data Cleaning\n",
    "7. Model creation\n",
    "    - Spliting the dataset\n",
    "    - Visualizing the attributes\n",
    "    - Vectorization\n",
    "    - Final Model Creation\n",
    "    - Inference\n",
    "8. Additional! Creating a Python function\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************\n",
    "### Abstract\n",
    "Summarization is the task of condensing a piece of text to a shorter version, reducing the size of the initial text while at the same time preserving key informational elements and the meaning of content. Since manual text summarization is a time expensive and generally laborious task, the automatization of the task is gaining increasing popularity and therefore constitutes a strong motivation for academic research.\n",
    "\n",
    "There are important applications for text summarization in various NLP related tasks such as text classification, question answering, legal texts summarization, news summarization, and headline generation. Moreover, the generation of summaries can be integrated into these systems as an intermediate stage which helps to reduce the length of the document.\n",
    "\n",
    "\n",
    "### Goal\n",
    "The goal of this project is to create a model which will summarize the articles given by the users. In addition I will be creating a Python function to make the model more user friendly!\n",
    "***********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,LSTM,Bidirectional,Flatten,Dropout,BatchNormalization,Embedding,Input,TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Data preprocessing is an important step in the data mining process. The phrase \" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [01:15,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "all_data=[]\n",
    "Articles_with_stopwords=[]\n",
    "Articles_without_stopwords=[]\n",
    "Summaries=[]\n",
    "stop_words=set(stopwords.words('english'))\n",
    "for d,path,filenames in tqdm(os.walk('D:\\ML\\Text Summarization\\Dataset\\BBC News Summary')):\n",
    "    for file in filenames:\n",
    "        if os.path.isfile(d+'/'+file):\n",
    "            if('Summaries' in d+'/'+file):\n",
    "                with open(d+'/'+file,'r',errors='ignore') as f:\n",
    "                    summary=''.join([i.rstrip() for i in f.readlines()])\n",
    "                    Summaries.append(summary)\n",
    "                    f.close()\n",
    "            else:\n",
    "                with open(d+'/'+file,'r',errors='ignore') as f:\n",
    "                    Article=''.join([i.rstrip() for i in f.readlines()])\n",
    "                    Articles_with_stopwords.append(Article)\n",
    "                    Articles_without_stopwords.append(' '.join([w for w in Article.split() if w not in stop_words]))\n",
    "                    f.close()\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4450 4450 4450\n"
     ]
    }
   ],
   "source": [
    "print(len(Articles_with_stopwords),len(Articles_without_stopwords),len(Summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles without stop words</th>\n",
       "      <th>Article with stop words</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profitQuarterly pro...</td>\n",
       "      <td>Ad sales boost Time Warner profitQuarterly pro...</td>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains Greenspan speechThe dollar hit hi...</td>\n",
       "      <td>Dollar gains on Greenspan speechThe dollar has...</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claimThe owners em...</td>\n",
       "      <td>Yukos unit buyer faces loan claimThe owners of...</td>\n",
       "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profitsBritish Airwa...</td>\n",
       "      <td>High fuel prices hit BA's profitsBritish Airwa...</td>\n",
       "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts DomecqShares UK dri...</td>\n",
       "      <td>Pernod takeover talk lifts DomecqShares in UK ...</td>\n",
       "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Articles without stop words  \\\n",
       "0  Ad sales boost Time Warner profitQuarterly pro...   \n",
       "1  Dollar gains Greenspan speechThe dollar hit hi...   \n",
       "2  Yukos unit buyer faces loan claimThe owners em...   \n",
       "3  High fuel prices hit BA's profitsBritish Airwa...   \n",
       "4  Pernod takeover talk lifts DomecqShares UK dri...   \n",
       "\n",
       "                             Article with stop words  \\\n",
       "0  Ad sales boost Time Warner profitQuarterly pro...   \n",
       "1  Dollar gains on Greenspan speechThe dollar has...   \n",
       "2  Yukos unit buyer faces loan claimThe owners of...   \n",
       "3  High fuel prices hit BA's profitsBritish Airwa...   \n",
       "4  Pernod takeover talk lifts DomecqShares in UK ...   \n",
       "\n",
       "                                             Summary  \n",
       "0  TimeWarner said fourth quarter sales rose 2% t...  \n",
       "1  The dollar has hit its highest level against t...  \n",
       "2  Yukos' owner Menatep Group says it will ask Ro...  \n",
       "3  Rod Eddington, BA's chief executive, said the ...  \n",
       "4  Pernod has reduced the debt it took on to fund...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame({'Articles without stop words':Articles_without_stopwords,'Article with stop words': Articles_with_stopwords,'Summary':Summaries})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ad sales boost Time Warner profitQuarterly profits US media giant TimeWarner jumped 76% $1.13bn (Â£600m) three months December, $639m year-earlier.The firm, one biggest investors Google, benefited sales high-speed internet connections higher advert sales. TimeWarner said fourth quarter sales rose 2% $11.1bn $10.9bn. Its profits buoyed one-off gains offset profit dip Warner Bros, less users AOL.Time Warner said Friday owns 8% search-engine Google. But internet business, AOL, mixed fortunes. It lost 464,000 subscribers fourth quarter profits lower preceding three quarters. However, company said AOL\\'s underlying profit exceptional items rose 8% back stronger internet advertising revenues. It hopes increase subscribers offering online service free TimeWarner internet customers try sign AOL\\'s existing customers high-speed broadband. TimeWarner also restate 2000 2003 results following probe US Securities Exchange Commission (SEC), close concluding.Time Warner\\'s fourth quarter profits slightly better analysts\\' expectations. But film division saw profits slump 27% $284m, helped box-office flops Alexander Catwoman, sharp contrast year-earlier, third final film Lord Rings trilogy boosted results. For full-year, TimeWarner posted profit $3.36bn, 27% 2003 performance, revenues grew 6.4% $42.09bn. \"Our financial performance strong, meeting exceeding full-year objectives greatly enhancing flexibility,\" chairman chief executive Richard Parsons said. For 2005, TimeWarner projecting operating earnings growth around 5%, also expects higher revenue wider profit margins.TimeWarner restate accounts part efforts resolve inquiry AOL US market regulators. It already offered pay $300m settle charges, deal review SEC. The company said unable estimate amount needed set aside legal reserves, previously set $500m. It intends adjust way accounts deal German music publisher Bertelsmann\\'s purchase stake AOL Europe, reported advertising revenue. It book sale stake AOL Europe loss value stake.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Articles without stop words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad sales boost Time Warner profitQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
      "---------------------------------------------\n",
      "Dollar gains on Greenspan speechThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"Worries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
      "---------------------------------------------\n",
      "Yukos unit buyer faces loan claimThe owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $900m (Â£479m) loan.State-owned Rosneft bought the Yugansk unit for $9.3bn in a sale forced by Russia to part settle a $27.5bn tax claim against Yukos. Yukos' owner Menatep Group says it will ask Rosneft to repay a loan that Yugansk had secured on its assets. Rosneft already faces a similar $540m repayment demand from foreign banks. Legal experts said Rosneft's purchase of Yugansk would include such obligations. \"The pledged assets are with Rosneft, so it will have to pay real money to the creditors to avoid seizure of Yugansk assets,\" said Moscow-based US lawyer Jamie Firestone, who is not connected to the case. Menatep Group's managing director Tim Osborne told the Reuters news agency: \"If they default, we will fight them where the rule of law exists under the international arbitration clauses of the credit.\"Rosneft officials were unavailable for comment. But the company has said it intends to take action against Menatep to recover some of the tax claims and debts owed by Yugansk. Yukos had filed for bankruptcy protection in a US court in an attempt to prevent the forced sale of its main production arm. The sale went ahead in December and Yugansk was sold to a little-known shell company which in turn was bought by Rosneft. Yukos claims its downfall was punishment for the political ambitions of its founder Mikhail Khodorkovsky and has vowed to sue any participant in the sale.\n",
      "---------------------------------------------\n",
      "High fuel prices hit BA's profitsBritish Airways has blamed high fuel prices for a 40% drop in profits.Reporting its results for the three months to 31 December 2004, the airline made a pre-tax profit of Â£75m ($141m) compared with Â£125m a year earlier. Rod Eddington, BA's chief executive, said the results were \"respectable\" in a third quarter when fuel costs rose by Â£106m or 47.3%. BA's profits were still better than market expectation of Â£59m, and it expects a rise in full-year revenues.To help offset the increased price of aviation fuel, BA last year introduced a fuel surcharge for passengers.In October, it increased this from Â£6 to Â£10 one-way for all long-haul flights, while the short-haul surcharge was raised from Â£2.50 to Â£4 a leg. Yet aviation analyst Mike Powell of Dresdner Kleinwort Wasserstein says BA's estimated annual surcharge revenues - Â£160m - will still be way short of its additional fuel costs - a predicted extra Â£250m. Turnover for the quarter was up 4.3% to Â£1.97bn, further benefiting from a rise in cargo revenue. Looking ahead to its full year results to March 2005, BA warned that yields - average revenues per passenger - were expected to decline as it continues to lower prices in the face of competition from low-cost carriers. However, it said sales would be better than previously forecast. \"For the year to March 2005, the total revenue outlook is slightly better than previous guidance with a 3% to 3.5% improvement anticipated,\" BA chairman Martin Broughton said. BA had previously forecast a 2% to 3% rise in full-year revenue.It also reported on Friday that passenger numbers rose 8.1% in January. Aviation analyst Nick Van den Brul of BNP Paribas described BA's latest quarterly results as \"pretty modest\". \"It is quite good on the revenue side and it shows the impact of fuel surcharges and a positive cargo development, however, operating margins down and cost impact of fuel are very strong,\" he said. Since the 11 September 2001 attacks in the United States, BA has cut 13,000 jobs as part of a major cost-cutting drive. \"Our focus remains on reducing controllable costs and debt whilst continuing to invest in our products,\" Mr Eddington said. \"For example, we have taken delivery of six Airbus A321 aircraft and next month we will start further improvements to our Club World flat beds.\" BA's shares closed up four pence at 274.5 pence.\n",
      "---------------------------------------------\n",
      "Pernod takeover talk lifts DomecqShares in UK drinks and food firm Allied Domecq have risen on speculation that it could be the target of a takeover by France's Pernod Ricard.Reports in the Wall Street Journal and the Financial Times suggested that the French spirits firm is considering a bid, but has yet to contact its target. Allied Domecq shares in London rose 4% by 1200 GMT, while Pernod shares in Paris slipped 1.2%. Pernod said it was seeking acquisitions but refused to comment on specifics.Pernod's last major purchase was a third of US giant Seagram in 2000, the move which propelled it into the global top three of drinks firms. The other two-thirds of Seagram was bought by market leader Diageo. In terms of market value, Pernod - at 7.5bn euros ($9.7bn) - is about 9% smaller than Allied Domecq, which has a capitalisation of Â£5.7bn ($10.7bn; 8.2bn euros). Last year Pernod tried to buy Glenmorangie, one of Scotland's premier whisky firms, but lost out to luxury goods firm LVMH. Pernod is home to brands including Chivas Regal Scotch whisky, Havana Club rum and Jacob's Creek wine. Allied Domecq's big names include Malibu rum, Courvoisier brandy, Stolichnaya vodka and Ballantine's whisky - as well as snack food chains such as Dunkin' Donuts and Baskin-Robbins ice cream. The WSJ said that the two were ripe for consolidation, having each dealt with problematic parts of their portfolio. Pernod has reduced the debt it took on to fund the Seagram purchase to just 1.8bn euros, while Allied has improved the performance of its fast-food chains.\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(data['Article with stop words'][i])\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.Time Warner's fourth quarter profits were slightly better than analysts' expectations.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Data cleansing or data cleaning is the process of detecting and correcting corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    text=' '.join([contraction_mapping[i] if i in contraction_mapping.keys() else i for i in text.split()])\n",
    "    text=re.sub(r'\\(.*\\)',\"\",text)\n",
    "    text=re.sub(\"'s\",\"\",text)\n",
    "    text=re.sub('\"','',text)\n",
    "    text=' '.join([i for i in text.split() if i.isalpha()])\n",
    "    text=re.sub('[^a-zA-Z]',\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Article with stop words']=data['Article with stop words'].apply(clean_text)\n",
    "data['Articles without stop words']=data['Articles without stop words'].apply(clean_text)\n",
    "data['Summary']=data['Summary'].apply(clean_text)\n",
    "data['Summary']='<START> '+data['Summary']+' <END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ad sales boost time warner profitquarterly profits at us media giant timewarner jumped to which is close to warner fourth quarter profits were slightly better than but its film division saw profits slump to helped by flops alexander and a sharp contrast to when the third and final film in the lord of the rings trilogy boosted for the timewarner posted a profit of up from its while revenues grew to our financial performance was meeting or exceeding all of our objectives and greatly enhancing our chairman and chief executive richard parsons for timewarner is projecting operating earnings growth of around and also expects higher revenue and wider profit is to restate its accounts as part of efforts to resolve an inquiry into aol by us market it has already offered to pay to settle in a deal that is under review by the the company said it was unable to estimate the amount it needed to set aside for legal which it previously set at it intends to adjust the way it accounts for a deal with german music publisher bertelsmann purchase of a stake in aol which it had reported as advertising it will now book the sale of its stake in aol europe as a loss on the value of that'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Article with stop words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ad sales boost time warner profitquarterly profits us media giant timewarner jumped close warner fourth quarter profits slightly better but film division saw profits slump helped flops alexander sharp contrast third final film lord rings trilogy boosted for timewarner posted profit revenues grew our financial performance meeting exceeding objectives greatly enhancing chairman chief executive richard parsons for timewarner projecting operating earnings growth around also expects higher revenue wider profit restate accounts part efforts resolve inquiry aol us market it already offered pay settle deal review the company said unable estimate amount needed set aside legal previously set it intends adjust way accounts deal german music publisher bertelsmann purchase stake aol reported advertising it book sale stake aol europe loss value'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Articles without stop words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> timewarner said fourth quarter sales rose to from the timewarner posted a profit of up from its while revenues grew to profits at us media giant timewarner jumped to for the three months to from the company said aol underlying profit before exceptional items rose on the back of stronger internet advertising profits were buoyed by gains which offset a profit dip at warner and less users for timewarner is projecting operating earnings growth of around and also expects higher revenue and wider profit lost subscribers in the fourth quarter profits were lower than in the preceding three warner fourth quarter profits were slightly better than <END>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************\n",
    "## Model Creation\n",
    "\n",
    "**Training and Testing Dataset Spliting using the `train_test_split`**\n",
    "  \n",
    "  * Immporting the library from the sklearn.model_selection\n",
    "  * Split the dataset into 70:30 ratio\n",
    "  * x_train and y_train are the trainning datasets\n",
    "  * x_test and y_test are the testing datasets\n",
    "  * After the spliting of the datasets the model is ready to be prepared!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3115 3115\n",
      "1335 1335\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,Y_train,Y_val=train_test_split(data['Article with stop words'],data['Summary'],test_size=0.3,random_state=29)\n",
    "print(len(X_train),len(Y_train))\n",
    "print(len(X_val),len(Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Dataset based on Article and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "art_len=[len(i.split()) for i in X_train]\n",
    "sum_len=[len(i.split()) for i in Y_train]\n",
    "plt.hist(art_len,bins=100)\n",
    "plt.title('Article')\n",
    "plt.show()\n",
    "plt.hist(sum_len,bins=50)\n",
    "plt.title('Summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_art_len=500\n",
    "max_sum_len=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization on the dataset\n",
    "Vectorization is a technique by which you can make your code execute fast. It is a very interesting and important way to optimize algorithms when you are implementing it from scratch.\n",
    "Now, with the help of highly optimized numerical linear algebra libraries in C/C++, Octave/Matlab, Python, …etc. We can make our code run efficiently.\n",
    "In machine learning, there’s a concept of an optimization algorithm that tries to reduce the error and computes to get the best parameters for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tokenizer=Tokenizer(oov_token='<UNK>')\n",
    "article_tokenizer.fit_on_texts(X_train)\n",
    "tokenized_X_train=article_tokenizer.texts_to_sequences(X_train)\n",
    "tokenized_X_val=article_tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_vocab_size=len(article_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train=pad_sequences(tokenized_X_train,maxlen=max_art_len,padding='post',truncating='post')\n",
    "padded_X_val=pad_sequences(tokenized_X_val,maxlen=max_art_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3115, 500) (1335, 500)\n"
     ]
    }
   ],
   "source": [
    "print(padded_X_train.shape,padded_X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tokenizer=Tokenizer(oov_token='<UNK>')\n",
    "summary_tokenizer.fit_on_texts(Y_train)\n",
    "tokenized_Y_train=summary_tokenizer.texts_to_sequences(Y_train)\n",
    "tokenized_Y_val=summary_tokenizer.texts_to_sequences(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vocab_size=len(summary_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_Y_train=pad_sequences(tokenized_Y_train,maxlen=max_sum_len,padding='post',truncating='post')\n",
    "padded_Y_val=pad_sequences(tokenized_Y_val,maxlen=max_sum_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3115, 100) (1335, 100)\n"
     ]
    }
   ],
   "source": [
    "print(padded_Y_train.shape,padded_Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_art_index=article_tokenizer.index_word\n",
    "reverse_sum_index=summary_tokenizer.index_word\n",
    "sum_wordindex=summary_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Deployment\n",
    "Before deploying the main model let us discuss about the Neural Network and to be precise the Long Short Term Memory RNN.\n",
    "\n",
    "**Neural Network** : A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature.\n",
    "\n",
    "**Reccurrent Neural Network** : RNNs are a powerful and robust type of neural network, and belong to the most promising algorithms in use because it is the only one with an internal memory.\n",
    "\n",
    "Like many other deep learning algorithms, recurrent neural networks are relatively old. They were initially created in the 1980’s, but only in recent years have we seen their true potential. An increase in computational power along with the the massive amounts of data that we now have to work with, and the invention of long short-term memory (LSTM) in the 1990s, has really brought RNNs to the foreground.\n",
    "\n",
    "**Long Short Term Memory** : Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions, RNN’s may leave out important information from the beginning.\n",
    "\n",
    "During back propagation, recurrent neural networks suffer from the vanishing gradient problem. Gradients are values used to update a neural networks weights. The vanishing gradient problem is when the gradient shrinks as it back propagates through time. If a gradient value becomes extremely small, it doesn’t contribute too much learning.\n",
    "\n",
    "LSTM ’s and GRU’s were created as the solution to short-term memory. They have internal mechanisms called gates that can regulate the flow of information.\n",
    "\n",
    "let's deploy the model using LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Encoder_Embedding_layer (Embed  (None, 500, 100)    2425600     ['input_1[0][0]']                \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " Decoder_Embedding_layer (Embed  (None, None, 100)   1577600     ['input_2[0][0]']                \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " Encoder_LSTM1 (LSTM)           [(None, 500, 300),   481200      ['Encoder_Embedding_layer[0][0]']\n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " Decoder_LSTM1 (LSTM)           [(None, None, 300),  481200      ['Decoder_Embedding_layer[0][0]',\n",
      "                                 (None, 300),                     'Encoder_LSTM1[0][1]',          \n",
      "                                 (None, 300)]                     'Encoder_LSTM1[0][2]']          \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 15776)  4748576    ['Decoder_LSTM1[0][0]']          \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,714,176\n",
      "Trainable params: 9,714,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs=Input(shape=(max_art_len,))\n",
    "encoder_emb=Embedding(art_vocab_size,100,trainable=True,name='Encoder_Embedding_layer')(encoder_inputs)\n",
    "encoder_lstm1=LSTM(300,return_sequences=True,return_state=True,name='Encoder_LSTM1')\n",
    "enclstm1_outputs,enclstm1_h,enclstm1_c=encoder_lstm1(encoder_emb)\n",
    "\n",
    "\n",
    "decoder_inputs=Input(shape=(None,))\n",
    "decoder_em=Embedding(sum_vocab_size,100,trainable=True,name='Decoder_Embedding_layer')\n",
    "decoder_emb=decoder_em(decoder_inputs)\n",
    "\n",
    "decoder_lstm1=LSTM(300,return_sequences=True,return_state=True,name='Decoder_LSTM1')\n",
    "declstm1_output,declstm1_h,declstm1_c=decoder_lstm1(decoder_emb,initial_state=[enclstm1_h,enclstm1_c])\n",
    "\n",
    "output_layer=TimeDistributed(Dense(sum_vocab_size,activation='softmax',name='softmax'))\n",
    "output=output_layer(declstm1_output)\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 469s 10s/step - loss: 7.5577 - val_loss: 6.6819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25745888a30>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padded_X_train,padded_Y_train[:,:-1]],padded_Y_train[:,1:],\n",
    "          epochs=1,\n",
    "          validation_data=([padded_X_val,padded_Y_val[:,:-1]],padded_Y_val[:,1:]),\n",
    "          batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "An ML lifecycle can be broken up into two main, distinct parts. The first is the training phase, in which an ML model is created or “trained” by running a specified subset of data into the model. ML inference is the second phase, in which the model is put into action on live data to produce actionable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "infencoder_model=Model(inputs=[encoder_inputs],outputs=[enclstm1_outputs,enclstm1_h,enclstm1_c])\n",
    "\n",
    "infdecoder_model_state_input_h=Input(shape=(300,),name='infdec_I1')\n",
    "infdecoder_model_state_input_c=Input(shape=(300,),name='infdec_I2')\n",
    "\n",
    "infdeclstm1_output,infdec_h,infdec_c=decoder_lstm1(decoder_emb,initial_state=[infdecoder_model_state_input_h,\n",
    "                                                                                                infdecoder_model_state_input_c\n",
    "                                                                                               ])\n",
    "\n",
    "infdec_output=output_layer(infdeclstm1_output)                         \n",
    "\n",
    "infdecoder_model=Model(inputs=[decoder_inputs]+[infdecoder_model_state_input_h,infdecoder_model_state_input_c],\n",
    "                       outputs=[infdec_output]+[infdec_h,infdec_c])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " Encoder_Embedding_layer (Em  (None, 500, 100)         2425600   \n",
      " bedding)                                                        \n",
      "                                                                 \n",
      " Encoder_LSTM1 (LSTM)        [(None, 500, 300),        481200    \n",
      "                              (None, 300),                       \n",
      "                              (None, 300)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,906,800\n",
      "Trainable params: 2,906,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "infencoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(infencoder_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Decoder_Embedding_layer (Embed  (None, None, 100)   1577600     ['input_2[0][0]']                \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " infdec_I1 (InputLayer)         [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " infdec_I2 (InputLayer)         [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " Decoder_LSTM1 (LSTM)           [(None, None, 300),  481200      ['Decoder_Embedding_layer[0][0]',\n",
      "                                 (None, 300),                     'infdec_I1[0][0]',              \n",
      "                                 (None, 300)]                     'infdec_I2[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 15776)  4748576    ['Decoder_LSTM1[1][0]']          \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,807,376\n",
      "Trainable params: 6,807,376\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "infdecoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(infdecoder_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************\n",
    "### Additional! Creating a Python function for making the model user friendly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(inp_seq):\n",
    "    \n",
    "    enc_out,enc_h,enc_c=infencoder_model.predict(inp_seq)\n",
    "    \n",
    "    tar_seq=np.zeros((1,1))\n",
    "    tar_seq[0,0]=sum_wordindex['start']\n",
    "    \n",
    "    stop_loop=False\n",
    "    decoded_string=''\n",
    "    \n",
    "    while not stop_loop:\n",
    "       \n",
    "        dec_out,dec_h,dec_c=infdecoder_model.predict([tar_seq]+[enc_h,enc_c])\n",
    "        \n",
    "        tar_token_index=np.argmax(dec_out[0,-1,:])\n",
    "        tar_token_word=sum_wordindex[tar_token_index]\n",
    "        \n",
    "        if tar_token_word =='end' or len(decoded_string)>=max_art_len:\n",
    "            \n",
    "            stop_loop=True\n",
    "        else:\n",
    "            decoded_string+=tar_token_word\n",
    "            \n",
    "            tar_seq=np.zeros((1,1))\n",
    "            tar_seq[0,0]=tar_token_index\n",
    "            \n",
    "            \n",
    "            enc_h=dec_h\n",
    "            enc_c=dec_c\n",
    "            \n",
    "    return decoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2art(inp_seq):\n",
    "    \n",
    "    art=''\n",
    "    \n",
    "    for i in range(len(inp_seq)):\n",
    "        \n",
    "        if inp_seq[i]==0:\n",
    "            break\n",
    "        art+=reverse_art_index[inp_seq[i]]+' '\n",
    "        \n",
    "    return art\n",
    "\n",
    "\n",
    "def seq2sum(inp_seq):\n",
    "    \n",
    "    summary=''\n",
    "    \n",
    "    for i in range(len(inp_seq)):\n",
    "        \n",
    "        if inp_seq[i]==0:\n",
    "            break\n",
    "        word=reverse_sum_index[inp_seq[i]]\n",
    "        summary+=word+' '\n",
    "            \n",
    "    return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *******************\n",
    "## Conclusion\n",
    "* Text Summarization models are one of the hot topics of the deep learning model deployment.\n",
    "* **Recurrent Neural Network** is the best choice for this kind of topics.\n",
    "* Here we have used **Long Short Term Memory**, which shows the validation loss of 6.49 only, which shows the accuracy of the model.\n",
    "* After inference, the model's validation loss decreased, and it is only 4.58.\n",
    "* LSTM on Inference is the best model to be fitted with this dataset.\n",
    "* Hence, the **Text Summarization Model** successfully deployed and working properly.\n",
    "* Last but not the least, the python function is made with **Sequence to Sequence Model**, which makes the function user friendly!\n",
    "\n",
    "************************************\n",
    "### Hope this project will help you. Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
